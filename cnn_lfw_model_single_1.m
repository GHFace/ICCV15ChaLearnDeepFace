function [net, info] = cnn_lfw_model_single_1(mm,n,str0)
% mm is set to []
% str0 is the prefix of the file name
% n is the index for LFW training set splits, the range of n is (0, 9)


% run(fullfile(fileparts(mfilename('fullpath')),'..','matlab','vl_setupnn.m')) ;
opts.dataDir = fullfile('data','lfw') ;
opts.expDir = fullfile('data',sprintf('lfw-model%d-%s',n,str0)) ;
opts.imdbPath = fullfile('data','lfw-baseline', sprintf('trn%d.mat',n));
opts.train.batchSize = 100 ;
opts.train.numEpochs = 50 ;
opts.train.continue = true ;
opts.train.useGpu = false ;
opts.train.learningRate = 0.001 ;
opts.train.expDir = opts.expDir ;
opts = vl_argparse(opts,mm) ;

% --------------------------------------------------------------------
%                                                         Prepare data
% --------------------------------------------------------------------

load(opts.imdbPath) ;
system(['rm ',opts.expDir, '/net-*'])

% Define a network similar to LeNet
scal=1;
net.layers = {} ;
% Block 1
net.layers{end+1} = struct('type', 'conv', ...
                           'filters', 0.01/scal * randn(5, 5, 3, 12, 'single'), ...
                           'biases', zeros(1, 12, 'single'), ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'filtersLearningRate', 1, ...
                           'biasesLearningRate', 2, ...
                           'filtersWeightDecay', 1, ...
                           'biasesWeightDecay', 0) ;
net.layers{end+1} = struct('type', 'relu') ;
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [2 2], ...
                           'stride', 2, ...
                           'pad', 0) ;
net.layers{end+1} = struct('type', 'conv', ...
                           'filters', 0.01/scal * randn(4, 4, 12, 24, 'single'), ...
                           'biases', zeros(1, 24, 'single'), ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'filtersLearningRate', 1, ...
                           'biasesLearningRate', 2, ...
                           'filtersWeightDecay', 1, ...
                           'biasesWeightDecay', 0) ;
net.layers{end+1} = struct('type', 'relu') ;
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [2 2], ...
                           'stride', 2, ...
                           'pad', 0) ;
net.layers{end+1} = struct('type', 'conv', ...
                           'filters', 0.01/scal * randn(3, 3, 24, 32, 'single'), ...
                           'biases', zeros(1, 32, 'single'), ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'filtersLearningRate', 1, ...
                           'biasesLearningRate', 2, ...
                           'filtersWeightDecay', 1, ...
                           'biasesWeightDecay', 0) ;
net.layers{end+1} = struct('type', 'relu') ;
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [2 2], ...
                           'stride', 2, ...
                           'pad', 0) ;
net.layers{end+1} = struct('type', 'conv', ...
                           'filters', 0.01/scal * randn(5, 5, 32, 160, 'single'), ...
                           'biases', zeros(1, 160, 'single'), ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'filtersLearningRate', 1, ...
                           'biasesLearningRate', 2, ...
                           'filtersWeightDecay', 1, ...
                           'biasesWeightDecay', 0) ;
                       
% net.layers{end+1} = struct('type', 'relu') ;
% net.layers{end+1} = struct('type', 'dropout', ...
% 'rate', 0.5) ;

net.layers{end+1} = struct('type', 'conv', ...
                           'filters', 0.01/scal * randn(1, 1, 160, length(imdb.meta.classes), 'single'), ...
                           'biases', zeros(1, length(imdb.meta.classes), 'single'), ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'filtersLearningRate', 1, ...
                           'biasesLearningRate', 2, ...
                           'filtersWeightDecay', 1, ...
                           'biasesWeightDecay', 0) ;
net.layers{end+1} = struct('type', 'softmaxloss') ;


% --------------------------------------------------------------------
%                                                                Train
% --------------------------------------------------------------------

% Take the mean out and make GPU if needed

% imdb.images.data = 0.2989 * imdb.images.data(:,:,1,:) + 0.5870 * imdb.images.data(:,:,2,:) + 0.1140 * imdb.images.data(:,:,3,:);

imdb.images.data = bsxfun(@minus, imdb.images.data, mean(imdb.images.data,4)) ;
if opts.train.useGpu
  imdb.images.data = gpuArray(imdb.images.data) ;
end

[net, info] = cnn_train(net, imdb, @getBatch, ...
    opts.train, ...
    'val', find(imdb.images.set == 3)) ;

% --------------------------------------------------------------------
function [im, labels] = getBatch(imdb, batch)
% --------------------------------------------------------------------
im = imdb.images.data(:,:,:,batch) ;
labels = imdb.images.labels(1,batch) ;
